{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057407ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a93e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"hacker_news\" dataset\n",
    "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bbe6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the tables in the \"hacker_news\" dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# Print names of all tables in the dataset (there are four!)\n",
    "for table in tables:  \n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53399365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"full\" table\n",
    "table_ref = dataset_ref.table(\"full\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
    "table.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five lines of the \"full\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first five entries in the \"by\" column of the \"full\" table\n",
    "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd70f98",
   "metadata": {},
   "source": [
    "exercise 2, 1 table, get access to crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc4afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"crime\" table\n",
    "table_ref = dataset_ref.table(\"crime\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Print information on all the columns in the \"crime\" table in the \"chicago_crime\" dataset\n",
    "print(table.schema)\n",
    "\n",
    "num_timestamp_fields = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f486e68",
   "metadata": {},
   "source": [
    "global air \n",
    "US , global_air_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33464c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Set up the query\n",
    "query_job = client.query(query)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "us_cities = query_job.to_dataframe()\n",
    "\n",
    "# What five cities have the most measurements?\n",
    "us_cities.city.value_counts().head()\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b79a282",
   "metadata": {},
   "source": [
    "very big datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9607c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\"\n",
    "        SELECT score, title\n",
    "        FROM `bigquery-public-data.hacker_news.full`\n",
    "        WHERE type = \"job\" \n",
    "        \"\"\"\n",
    "\n",
    "# Create a QueryJobConfig object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "# API request - dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ab60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the query if it's less than 1 MB\n",
    "ONE_MB = 1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 MB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "safe_query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c74ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the query if it's less than 1 GB\n",
    "ONE_GB = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 GB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "# Print average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940dbd72",
   "metadata": {},
   "source": [
    "--- exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46380dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.sql.ex2 import *\n",
    "print(\"Setup Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a2fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"openaq\" dataset\n",
    "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# Construct a reference to the \"global_air_quality\" table\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"global_air_quality\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e118dc6",
   "metadata": {},
   "source": [
    "unit= ppm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2e1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select countries with units of \"ppm\"\n",
    "first_query = \"\"\"\n",
    "        SELECT country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE unit = \"ppm\" \n",
    "        \"\"\" # Your code goes here\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "first_query_job = client.query(first_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "first_results = first_query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(first_results.head())\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ae9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select all columns where pollution levels are exactly 0\n",
    "zero_pollution_query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE value = 0 # Your code goes here\"\"\"\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(zero_pollution_query, job_config=safe_config)\n",
    "\n",
    "\n",
    "# API request - run the query and return a pandas DataFrame\n",
    "zero_pollution_results = query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "print(zero_pollution_results.head())\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b7afb",
   "metadata": {},
   "source": [
    "comments - group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e3675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select comments that received more than 10 replies\n",
    "query_popular = \"\"\"\n",
    "                SELECT parent, COUNT(id)\n",
    "                FROM `bigquery-public-data.hacker_news.comments`\n",
    "                GROUP BY parent\n",
    "                HAVING COUNT(id) > 10\n",
    "                \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query_popular, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "popular_comments = query_job.to_dataframe()\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "popular_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d146823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved version of earlier query, now with aliasing & improved readability\n",
    "query_improved = \"\"\"\n",
    "                 SELECT parent, COUNT(1) AS NumPosts\n",
    "                 FROM `bigquery-public-data.hacker_news.comments`\n",
    "                 GROUP BY parent\n",
    "                 HAVING COUNT(1) > 10\n",
    "                 \"\"\"\n",
    "\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query_improved, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "improved_df = query_job.to_dataframe()\n",
    "\n",
    "# Print the first five rows of the DataFrame\n",
    "improved_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2b2126",
   "metadata": {},
   "source": [
    "Note that there are two variables: parent and id.\n",
    "\n",
    "parent was passed to a GROUP BY command (in GROUP BY parent), and\n",
    "id was passed to an aggregate function (in COUNT(id))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5461c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bad = \"\"\"\n",
    "            SELECT author, parent, COUNT(id)\n",
    "            FROM `bigquery-public-data.hacker_news.comments`\n",
    "            GROUP BY parent\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63636db7",
   "metadata": {},
   "source": [
    "exercise 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.sql.ex3 import *\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"hacker_news\" dataset\n",
    "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# Construct a reference to the \"comments\" table\n",
    "table_ref = dataset_ref.table(\"comments\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"comments\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f778d11",
   "metadata": {},
   "source": [
    "posts 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116948c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select prolific commenters and post counts\n",
    "prolific_commenters_query = \"\"\"\n",
    "        SELECT author, COUNT(1) AS NumPosts\n",
    "        FROM `bigquery-public-data.hacker_news.comments`\n",
    "        GROUP BY author\n",
    "        HAVING COUNT(1) > 10000\n",
    "        \"\"\" # Your code goes here\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(prolific_commenters_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "prolific_commenters = query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(prolific_commenters.head())\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09831358",
   "metadata": {},
   "source": [
    "check deleted column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddea6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query here and figure out the answer\n",
    "query = \"\"\"\n",
    "                      SELECT COUNT(1) AS num_deleted_posts\n",
    "                      FROM `bigquery-public-data.hacker_news.comments`\n",
    "                      WHERE deleted = True\n",
    "                      \"\"\"\n",
    "# Your code goes here\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "deleted = query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(deleted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ebf31d",
   "metadata": {},
   "source": [
    "exract- order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c45aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to find out the number of accidents for each day of the week\n",
    "query = \"\"\"\n",
    "        SELECT COUNT(consecutive_number) AS num_accidents, \n",
    "               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n",
    "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "        GROUP BY day_of_week\n",
    "        ORDER BY num_accidents DESC\n",
    "        \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\n",
    "query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "accidents_by_day = query_job.to_dataframe()\n",
    "\n",
    "# Print the DataFrame\n",
    "accidents_by_day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcd3ee",
   "metadata": {},
   "source": [
    "world_bank_intl_education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a194b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"world_bank_intl_education\" dataset\n",
    "dataset_ref = client.dataset(\"world_bank_intl_education\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# Construct a reference to the \"international_education\" table\n",
    "table_ref = dataset_ref.table(\"international_education\")\n",
    "\n",
    "# API request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"international_education\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547d63c",
   "metadata": {},
   "source": [
    "To answer this question, consider only the rows in the dataset corresponding to indicator code SE.XPD.TOTL.GD.ZS, and write a query that returns the average value in the value column for each country in the dataset between the years 2010-2017 (including 2010 and 2017 in the average).\n",
    "\n",
    "Requirements:\n",
    "\n",
    "Your results should have the country name rather than the country code. You will have one row for each country.\n",
    "The aggregate function for average is AVG(). Use the name avg_ed_spending_pct for the column created by this aggregation.\n",
    "Order the results so the countries that spend the largest fraction of GDP on education show up first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c547bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "country_spend_pct_query = \"\"\"\n",
    "                          SELECT country_name, AVG(value) as avg_ed_spending_pct\n",
    "                          FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                          WHERE indicator_code= \"SE.XPD.TOTL.GD.ZS\" and year >= 2010 and year <= 2017\n",
    "                          GROUP BY country_name\n",
    "                          ORDER BY avg_ed_spending_pct DESC\n",
    "                          \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "country_spend_pct_query_job = client.query(country_spend_pct_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "country_spending_results = country_spend_pct_query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(country_spending_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445ad136",
   "metadata": {},
   "source": [
    " Identify interesting codes to explore¶\n",
    "The last question started by telling you to focus on rows with the code SE.XPD.TOTL.GD.ZS. But how would you find more interesting indicator codes to explore?\n",
    "\n",
    "There are 1000s of codes in the dataset, so it would be time consuming to review them all. But many codes are available for only a few countries. When browsing the options for different codes, you might restrict yourself to codes that are reported by many countries.\n",
    "\n",
    "Write a query below that selects the indicator code and indicator name for all codes with at least 175 rows in the year 2016.\n",
    "\n",
    "Requirements:\n",
    "\n",
    "You should have one row for each indicator code.\n",
    "The columns in your results should be called indicator_code, indicator_name, and num_rows.\n",
    "Only select codes with 175 or more rows in the raw database (exactly 175 rows would be included).\n",
    "To get both the indicator_code and indicator_name in your resulting DataFrame, you need to include both in your SELECT statement (in addition to a COUNT() aggregation). This requires you to include both in your GROUP BY clause.\n",
    "Order from results most frequent to least frequent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9964f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "code_count_query = \"\"\"\n",
    "                   SELECT indicator_code, indicator_name, COUNT(1) AS num_rows\n",
    "                   FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                   WHERE year = 2016\n",
    "                   GROUP BY indicator_name, indicator_code\n",
    "                   HAVING COUNT(1) >= 175\n",
    "                   ORDER BY COUNT(1) DESC\n",
    "                   \"\"\"\n",
    "\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "code_count_query_job = client.query(code_count_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "code_count_results = code_count_query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(code_count_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f69eb",
   "metadata": {},
   "source": [
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afc432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select the number of transactions per date, sorted by date\n",
    "query_with_CTE = \"\"\" \n",
    "                 WITH time AS \n",
    "                 (\n",
    "                     SELECT DATE(block_timestamp) AS trans_date\n",
    "                     FROM `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                 )\n",
    "                 SELECT COUNT(1) AS transactions,\n",
    "                        trans_date\n",
    "                 FROM time\n",
    "                 GROUP BY trans_date\n",
    "                 ORDER BY trans_date\n",
    "                 \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query_with_CTE, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "transactions_by_date = query_job.to_dataframe()\n",
    "\n",
    "# Print the first five rows\n",
    "transactions_by_date.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a4cce",
   "metadata": {},
   "source": [
    "Since they're returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f110f133",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_by_date.set_index('trans_date').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1fbc68",
   "metadata": {},
   "source": [
    "exercise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003d0f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up feedback system\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.sql.ex5 import *\n",
    "print(\"Setup Complete\")\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"chicago_taxi_trips\" dataset\n",
    "dataset_ref = client.dataset(\"chicago_taxi_trips\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af150d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the tables in the dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# Print names of all tables in the dataset (there is only one!)\n",
    "for table in tables:  \n",
    "    print(table.table_id)\n",
    "    \n",
    "table_name = 'taxi_trips'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e1cdeb",
   "metadata": {},
   "source": [
    "3) Determine when this data is from¶\n",
    "If the data is sufficiently old, we might be careful before assuming the data is still relevant to traffic patterns today. Write a query that counts the number of trips in each year.\n",
    "\n",
    "Your results should have two columns:\n",
    "\n",
    "year - the year of the trips\n",
    "num_trips - the number of trips in that year\n",
    "Hints:\n",
    "\n",
    "When using GROUP BY and ORDER BY, you should refer to the columns by the alias year that you set at the top of the SELECT query.\n",
    "The SQL code to SELECT the year from trip_start_timestamp is SELECT EXTRACT(YEAR FROM trip_start_timestamp)\n",
    "The FROM field can be a little tricky until you are used to it. The format is:\n",
    "A backick (the symbol `).\n",
    "The project name. In this case it is bigquery-public-data.\n",
    "A period.\n",
    "The dataset name. In this case, it is chicago_taxi_trips.\n",
    "A period.\n",
    "The table name. You used this as your answer in 1) Find the data.\n",
    "A backtick (the symbol `)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d812b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "rides_per_year_query = \"\"\"\n",
    "                       SELECT EXTRACT(YEAR FROM trip_start_timestamp) AS year, \n",
    "                              COUNT(1) AS num_trips\n",
    "                       FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                       GROUP BY year\n",
    "                       ORDER BY year\n",
    "                       \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "rides_per_year_query_job =client.query(rides_per_year_query, job_config=safe_config)\n",
    " # Your code goes here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "rides_per_year_result = rides_per_year_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "# View results\n",
    "print(rides_per_year_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9472ce56",
   "metadata": {},
   "source": [
    "4) Dive slightly deeper\n",
    "You'd like to take a closer look at rides from 2017. Copy the query you used above in rides_per_year_query into the cell below for rides_per_month_query. Then modify it in two ways:\n",
    "\n",
    "Use a WHERE clause to limit the query to data from 2017.\n",
    "Modify the query to extract the month rather than the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f06c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "rides_per_month_query = \"\"\"\n",
    "                       SELECT EXTRACT(MONTH FROM trip_start_timestamp) AS month,  \n",
    "                              COUNT(1) AS num_trips\n",
    "                       FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                       WHERE EXTRACT(YEAR FROM trip_start_timestamp)= 2017\n",
    "                       GROUP BY month\n",
    "                       ORDER BY month\n",
    "                       \"\"\"\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "rides_per_month_query_job = client.query(rides_per_month_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "rides_per_month_result = rides_per_month_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "# View results\n",
    "print(rides_per_month_result)\n",
    "\n",
    "# Check your answer\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66324700",
   "metadata": {},
   "source": [
    "Write the query¶\n",
    "It's time to step up the sophistication of your queries. Write a query that shows, for each hour of the day in the dataset, the corresponding number of trips and average speed.\n",
    "\n",
    "Your results should have three columns:\n",
    "\n",
    "hour_of_day - sort by this column, which holds the result of extracting the hour from trip_start_timestamp.\n",
    "num_trips - the count of the total number of trips in each hour of the day (e.g. how many trips were started between 6AM and 7AM, independent of which day it occurred on).\n",
    "avg_mph - the average speed, measured in miles per hour, for trips that started in that hour of the day. Average speed in miles per hour is calculated as 3600 * SUM(trip_miles) / SUM(trip_seconds). (The value 3600 is used to convert from seconds to hours.)\n",
    "Restrict your query to data meeting the following criteria:\n",
    "\n",
    "a trip_start_timestamp between 2017-01-01 and 2017-07-01\n",
    "trip_seconds > 0 and trip_miles > 0\n",
    "You will use a common table expression (CTE) to select just the relevant rides. Because this dataset is very big, this CTE should select only the columns you'll need to create the final output (though you won't actually create those in the CTE -- instead you'll create those in the later SELECT statement below the CTE).\n",
    "\n",
    "This is a much harder query than anything you've written so far. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bb2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "speeds_query = \"\"\"\n",
    "               WITH RelevantRides AS\n",
    "               (\n",
    "                   SELECT EXTRACT(HOUR FROM trip_start_timestamp) AS hour_of_day, \n",
    "                          trip_miles, \n",
    "                          trip_seconds\n",
    "                   FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                   WHERE trip_start_timestamp > '2017-01-01' AND \n",
    "                         trip_start_timestamp < '2017-07-01' AND \n",
    "                         trip_seconds > 0 AND \n",
    "                         trip_miles > 0\n",
    "               )\n",
    "               SELECT hour_of_day, \n",
    "                      COUNT(1) AS num_trips, \n",
    "                      3600 * SUM(trip_miles) / SUM(trip_seconds) AS avg_mph\n",
    "               FROM RelevantRides\n",
    "               GROUP BY hour_of_day\n",
    "               ORDER BY hour_of_day\n",
    "               \"\"\"\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "speeds_query_job = client.query(speeds_query, job_config=safe_config) # Your code here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "speeds_result = speeds_query_job.to_dataframe() # Your code here\n",
    "\n",
    "# View results\n",
    "print(speeds_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb207cfe",
   "metadata": {},
   "source": [
    "JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee7fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to determine the number of files per license, sorted by number of files\n",
    "query = \"\"\"\n",
    "        SELECT L.license, COUNT(1) AS number_of_files\n",
    "        FROM `bigquery-public-data.github_repos.sample_files` AS sf\n",
    "        INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n",
    "            ON sf.repo_name = L.repo_name\n",
    "        GROUP BY L.license\n",
    "        ORDER BY number_of_files DESC\n",
    "        \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and convert the results to a pandas DataFrame\n",
    "file_count_by_license = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c423e",
   "metadata": {},
   "source": [
    "stackoverflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7da23dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Create a \"Client\" object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a reference to the \"stackoverflow\" dataset\n",
    "dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eaf859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of available tables \n",
    "tables = list(client.list_tables(dataset))\n",
    "list_of_tables = [table.table_id for table in tables]  # Your code here\n",
    "\n",
    "# Print your answer\n",
    "print(list_of_tables)\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d3ed80",
   "metadata": {},
   "source": [
    "post_answer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"posts_answers\" table\n",
    "answers_table_ref = dataset_ref.table(\"posts_answers\")\n",
    "\n",
    "# API request - fetch the table\n",
    "answers_table = client.get_table(answers_table_ref)\n",
    "\n",
    "# Preview the first five lines of the \"posts_answers\" table\n",
    "client.list_rows(answers_table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcb711",
   "metadata": {},
   "source": [
    "LIKE\n",
    " You can also use % as a \"wildcard\" for any number of characters. So you can also get the third row with:\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM `bigquery-public-data.pet_records.pets` \n",
    "        WHERE Name LIKE '%ipl%'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "questions_query = \"\"\"\n",
    "                  SELECT id, title, owner_user_id\n",
    "                  FROM `bigquery-public-data.stackoverflow.posts_questions`\n",
    "                  WHERE tags LIKE '%bigquery%'\n",
    "                  \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# your quota, with the limit set to 1 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "questions_query_job = client.query(questions_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "questions_results = questions_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "# Preview results\n",
    "print(questions_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddce28e",
   "metadata": {},
   "source": [
    "4) Your first join¶\n",
    "Now that you have a query to select questions on any given topic (in this case, you chose \"bigquery\"), you can find the answers to those questions with a JOIN.\n",
    "\n",
    "Write a query that returns the id, body and owner_user_id columns from the posts_answers table for answers to \"bigquery\"-related questions.\n",
    "\n",
    "You should have one row in your results for each answer to a question that has \"bigquery\" in the tags.\n",
    "Remember you can get the tags for a question from the tags column in the posts_questions table.\n",
    "Here's a reminder of what a JOIN looked like in the tutorial:\n",
    "\n",
    "query = \"\"\"\n",
    "        SELECT p.Name AS Pet_Name, o.Name AS Owner_Name\n",
    "        FROM `bigquery-public-data.pet_records.pets` as p\n",
    "        INNER JOIN `bigquery-public-data.pet_records.owners` as o \n",
    "            ON p.ID = o.Pet_ID\n",
    "        \"\"\"\n",
    "It may be useful to scroll up and review the first several rows of the posts_answers and posts_questions tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "answers_query = \"\"\"\n",
    "        SELECT a.id, a.body, a.owner_user_id , q.tags \n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q \n",
    "        INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "            ON q.id = a.parent_id\n",
    "        WHERE q.tags LIKE '%bigquery%'\n",
    "        \"\"\"\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=27*10**10)\n",
    "answers_query_job = client.query(answers_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "answers_results = answers_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "# Preview results\n",
    "print(answers_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf27b69",
   "metadata": {},
   "source": [
    "You have the merge you need. But you want a list of users who have answered many questions... which requires more work beyond your previous result.\n",
    "\n",
    "Write a new query that has a single row for each user who answered at least one question with a tag that includes the string \"bigquery\". Your results should have two columns:\n",
    "\n",
    "user_id - contains the owner_user_id column from the posts_answers table\n",
    "number_of_answers - contains the number of answers the user has written to \"bigquery\"-related questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b48fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "bigquery_experts_query = \"\"\"\n",
    "                         SELECT a.owner_user_id AS user_id, COUNT(1) AS number_of_answers\n",
    "                         FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "                         INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "                             ON q.id = a.parent_Id\n",
    "                         WHERE q.tags LIKE '%bigquery%'\n",
    "                         GROUP BY a.owner_user_id\n",
    "                         \"\"\"\n",
    "\n",
    "# Set up the query\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "bigquery_experts_query_job = client.query(bigquery_experts_query, job_config=safe_config)\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "bigquery_experts_results = bigquery_experts_query_job.to_dataframe()\n",
    "\n",
    "# Preview results\n",
    "print(bigquery_experts_results.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
